{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "MINI_PROJECT_Colab_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSlxWr4Nntb7"
      },
      "source": [
        "<a id=\"1\"></a> <br>\n",
        "## Import Libraries and the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2021-06-12T14:11:56.868883Z",
          "iopub.status.busy": "2021-06-12T14:11:56.868276Z",
          "iopub.status.idle": "2021-06-12T14:12:03.259402Z",
          "shell.execute_reply": "2021-06-12T14:12:03.258655Z",
          "shell.execute_reply.started": "2021-06-12T14:11:56.868825Z"
        },
        "id": "aKpM_fM5ntb8"
      },
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, GRU\n",
        "from keras.layers import Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import itertools\n",
        "import random\n",
        "import os\n",
        "\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "execution": {
          "iopub.execute_input": "2021-06-12T14:12:03.264247Z",
          "iopub.status.busy": "2021-06-12T14:12:03.262941Z",
          "iopub.status.idle": "2021-06-12T14:12:04.522482Z",
          "shell.execute_reply": "2021-06-12T14:12:04.521322Z",
          "shell.execute_reply.started": "2021-06-12T14:12:03.264188Z"
        },
        "id": "El0rgjVzntb9"
      },
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/shee35/ML-Dataset/main/newcsv.csv', parse_dates=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maU4rjKqntb9"
      },
      "source": [
        "<a id=\"2\"></a> <br>\n",
        "## Descriptive Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-12T14:12:04.524227Z",
          "iopub.status.busy": "2021-06-12T14:12:04.523929Z",
          "iopub.status.idle": "2021-06-12T14:12:04.546158Z",
          "shell.execute_reply": "2021-06-12T14:12:04.545279Z",
          "shell.execute_reply.started": "2021-06-12T14:12:04.524197Z"
        },
        "id": "TBXi8nPXntb-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "dd3ad382-8e73-4201-ccf3-ebccf8956b7d"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dates</th>\n",
              "      <th>Current</th>\n",
              "      <th>Temporal</th>\n",
              "      <th>Spatial</th>\n",
              "      <th>Derived</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4/26/2020</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>74.360000</td>\n",
              "      <td>3.922437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4/27/2020</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>76.692308</td>\n",
              "      <td>4.016444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4/28/2020</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>86.000000</td>\n",
              "      <td>4.391603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4/29/2020</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>88.423077</td>\n",
              "      <td>4.489269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4/30/2020</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>92.346154</td>\n",
              "      <td>4.647394</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Dates  Current  Temporal    Spatial   Derived\n",
              "0  4/26/2020        3       3.0  74.360000  3.922437\n",
              "1  4/27/2020        3       3.0  76.692308  4.016444\n",
              "2  4/28/2020        3       3.0  86.000000  4.391603\n",
              "3  4/29/2020        3       3.0  88.423077  4.489269\n",
              "4  4/30/2020        3       3.0  92.346154  4.647394"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-12T14:12:04.547993Z",
          "iopub.status.busy": "2021-06-12T14:12:04.547471Z",
          "iopub.status.idle": "2021-06-12T14:12:04.555652Z",
          "shell.execute_reply": "2021-06-12T14:12:04.554469Z",
          "shell.execute_reply.started": "2021-06-12T14:12:04.547959Z"
        },
        "id": "Kj2cCrHkntcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d97aff-f6bd-4da9-8957-397e1ff4bf58"
      },
      "source": [
        "print(data.columns)\n",
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Dates', 'Current', 'Temporal', 'Spatial', 'Derived'], dtype='object')\n",
            "(521, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFNuK36-ntcB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "dff891e2-b606-45a2-c168-b9daac7df95e"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dates</th>\n",
              "      <th>Current</th>\n",
              "      <th>Temporal</th>\n",
              "      <th>Spatial</th>\n",
              "      <th>Derived</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4/26/2020</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>74.360000</td>\n",
              "      <td>3.922437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4/27/2020</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>76.692308</td>\n",
              "      <td>4.016444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4/28/2020</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>86.000000</td>\n",
              "      <td>4.391603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4/29/2020</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>88.423077</td>\n",
              "      <td>4.489269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4/30/2020</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>92.346154</td>\n",
              "      <td>4.647394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>9/24/2021</td>\n",
              "      <td>66771</td>\n",
              "      <td>66676.2</td>\n",
              "      <td>174665.769200</td>\n",
              "      <td>27615.043900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517</th>\n",
              "      <td>9/25/2021</td>\n",
              "      <td>66838</td>\n",
              "      <td>66729.4</td>\n",
              "      <td>174754.538500</td>\n",
              "      <td>27636.591170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>518</th>\n",
              "      <td>9/26/2021</td>\n",
              "      <td>66853</td>\n",
              "      <td>66773.8</td>\n",
              "      <td>174842.730800</td>\n",
              "      <td>27650.513130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>519</th>\n",
              "      <td>9/27/2021</td>\n",
              "      <td>66868</td>\n",
              "      <td>66812.2</td>\n",
              "      <td>174716.038500</td>\n",
              "      <td>27654.602250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>520</th>\n",
              "      <td>9/28/2021</td>\n",
              "      <td>66915</td>\n",
              "      <td>66849.0</td>\n",
              "      <td>174790.884600</td>\n",
              "      <td>27670.122930</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>521 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Dates  Current  Temporal        Spatial       Derived\n",
              "0    4/26/2020        3       3.0      74.360000      3.922437\n",
              "1    4/27/2020        3       3.0      76.692308      4.016444\n",
              "2    4/28/2020        3       3.0      86.000000      4.391603\n",
              "3    4/29/2020        3       3.0      88.423077      4.489269\n",
              "4    4/30/2020        3       3.0      92.346154      4.647394\n",
              "..         ...      ...       ...            ...           ...\n",
              "516  9/24/2021    66771   66676.2  174665.769200  27615.043900\n",
              "517  9/25/2021    66838   66729.4  174754.538500  27636.591170\n",
              "518  9/26/2021    66853   66773.8  174842.730800  27650.513130\n",
              "519  9/27/2021    66868   66812.2  174716.038500  27654.602250\n",
              "520  9/28/2021    66915   66849.0  174790.884600  27670.122930\n",
              "\n",
              "[521 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTNd7uT8ntcC"
      },
      "source": [
        "data['Derived']=data['Derived'].ewm(span=2).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcRWCzM-ntcD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "d2ec3af5-046d-427d-b787-b1e949eafef0"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dates</th>\n",
              "      <th>Current</th>\n",
              "      <th>Temporal</th>\n",
              "      <th>Spatial</th>\n",
              "      <th>Derived</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4/26/2020</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>74.360000</td>\n",
              "      <td>3.922437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4/27/2020</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>76.692308</td>\n",
              "      <td>3.992942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4/28/2020</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>86.000000</td>\n",
              "      <td>4.268938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4/29/2020</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>88.423077</td>\n",
              "      <td>4.417661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4/30/2020</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>92.346154</td>\n",
              "      <td>4.571449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>9/24/2021</td>\n",
              "      <td>66771</td>\n",
              "      <td>66676.2</td>\n",
              "      <td>174665.769200</td>\n",
              "      <td>27606.309926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517</th>\n",
              "      <td>9/25/2021</td>\n",
              "      <td>66838</td>\n",
              "      <td>66729.4</td>\n",
              "      <td>174754.538500</td>\n",
              "      <td>27626.497422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>518</th>\n",
              "      <td>9/26/2021</td>\n",
              "      <td>66853</td>\n",
              "      <td>66773.8</td>\n",
              "      <td>174842.730800</td>\n",
              "      <td>27642.507894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>519</th>\n",
              "      <td>9/27/2021</td>\n",
              "      <td>66868</td>\n",
              "      <td>66812.2</td>\n",
              "      <td>174716.038500</td>\n",
              "      <td>27650.570798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>520</th>\n",
              "      <td>9/28/2021</td>\n",
              "      <td>66915</td>\n",
              "      <td>66849.0</td>\n",
              "      <td>174790.884600</td>\n",
              "      <td>27663.605553</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>521 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Dates  Current  Temporal        Spatial       Derived\n",
              "0    4/26/2020        3       3.0      74.360000      3.922437\n",
              "1    4/27/2020        3       3.0      76.692308      3.992942\n",
              "2    4/28/2020        3       3.0      86.000000      4.268938\n",
              "3    4/29/2020        3       3.0      88.423077      4.417661\n",
              "4    4/30/2020        3       3.0      92.346154      4.571449\n",
              "..         ...      ...       ...            ...           ...\n",
              "516  9/24/2021    66771   66676.2  174665.769200  27606.309926\n",
              "517  9/25/2021    66838   66729.4  174754.538500  27626.497422\n",
              "518  9/26/2021    66853   66773.8  174842.730800  27642.507894\n",
              "519  9/27/2021    66868   66812.2  174716.038500  27650.570798\n",
              "520  9/28/2021    66915   66849.0  174790.884600  27663.605553\n",
              "\n",
              "[521 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2alqumxlntcD"
      },
      "source": [
        "data = data.drop(['Dates', 'Current','Temporal',\n",
        "       'Spatial'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdOMFxkuntcE"
      },
      "source": [
        "<a id=\"24\"></a> <br>\n",
        "## Fearure Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "_GTPxookE06p",
        "outputId": "afbcf55f-aa46-4272-a823-d0290f8c1a40"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Derived</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.922437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.992942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.268938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.417661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.571449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>27606.309926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517</th>\n",
              "      <td>27626.497422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>518</th>\n",
              "      <td>27642.507894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>519</th>\n",
              "      <td>27650.570798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>520</th>\n",
              "      <td>27663.605553</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>521 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Derived\n",
              "0        3.922437\n",
              "1        3.992942\n",
              "2        4.268938\n",
              "3        4.417661\n",
              "4        4.571449\n",
              "..            ...\n",
              "516  27606.309926\n",
              "517  27626.497422\n",
              "518  27642.507894\n",
              "519  27650.570798\n",
              "520  27663.605553\n",
              "\n",
              "[521 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-12T14:12:06.364188Z",
          "iopub.status.busy": "2021-06-12T14:12:06.363736Z",
          "iopub.status.idle": "2021-06-12T14:12:06.376947Z",
          "shell.execute_reply": "2021-06-12T14:12:06.376009Z",
          "shell.execute_reply.started": "2021-06-12T14:12:06.364145Z"
        },
        "id": "NK3H2Y3MntcE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14a94dbd-7f05-4f02-b450-1d3054d26fa3"
      },
      "source": [
        "# Feature Scaling\n",
        "sc = MinMaxScaler(feature_range=(0, 1))\n",
        "data_scaled= sc.fit_transform(data)\n",
        "data_scaled"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00000000e+00],\n",
              "       [2.54902191e-06],\n",
              "       [1.25273020e-05],\n",
              "       [1.79041888e-05],\n",
              "       [2.34641862e-05],\n",
              "       [3.19197943e-05],\n",
              "       [4.09707800e-05],\n",
              "       [4.96644007e-05],\n",
              "       [8.81714939e-05],\n",
              "       [1.07693853e-04],\n",
              "       [1.22981773e-04],\n",
              "       [1.37306776e-04],\n",
              "       [1.47948019e-04],\n",
              "       [1.58818583e-04],\n",
              "       [1.92408618e-04],\n",
              "       [2.14366337e-04],\n",
              "       [2.38506396e-04],\n",
              "       [2.62813925e-04],\n",
              "       [2.84785935e-04],\n",
              "       [3.15483462e-04],\n",
              "       [3.46070630e-04],\n",
              "       [3.75231827e-04],\n",
              "       [4.20123391e-04],\n",
              "       [4.53072612e-04],\n",
              "       [4.88453416e-04],\n",
              "       [5.52976189e-04],\n",
              "       [6.34699324e-04],\n",
              "       [7.08587308e-04],\n",
              "       [7.86193079e-04],\n",
              "       [8.76811770e-04],\n",
              "       [9.46268206e-04],\n",
              "       [1.03120472e-03],\n",
              "       [1.13176863e-03],\n",
              "       [1.25131153e-03],\n",
              "       [1.35384037e-03],\n",
              "       [1.46770927e-03],\n",
              "       [1.55969204e-03],\n",
              "       [1.64591232e-03],\n",
              "       [1.75599718e-03],\n",
              "       [1.86501914e-03],\n",
              "       [1.98689564e-03],\n",
              "       [2.12896331e-03],\n",
              "       [2.26881117e-03],\n",
              "       [2.37530503e-03],\n",
              "       [2.46010861e-03],\n",
              "       [2.59401925e-03],\n",
              "       [2.70136276e-03],\n",
              "       [2.79513686e-03],\n",
              "       [2.89172420e-03],\n",
              "       [2.99364251e-03],\n",
              "       [3.08111672e-03],\n",
              "       [3.16273869e-03],\n",
              "       [3.25630617e-03],\n",
              "       [3.37105763e-03],\n",
              "       [3.49856423e-03],\n",
              "       [3.62000368e-03],\n",
              "       [3.75650020e-03],\n",
              "       [3.88563563e-03],\n",
              "       [4.00539719e-03],\n",
              "       [4.12190817e-03],\n",
              "       [4.25453009e-03],\n",
              "       [4.40725984e-03],\n",
              "       [4.56346594e-03],\n",
              "       [4.74992586e-03],\n",
              "       [4.91516791e-03],\n",
              "       [5.09787536e-03],\n",
              "       [5.29061821e-03],\n",
              "       [5.49914819e-03],\n",
              "       [5.72697810e-03],\n",
              "       [6.00905092e-03],\n",
              "       [6.28688265e-03],\n",
              "       [6.58098794e-03],\n",
              "       [6.85714153e-03],\n",
              "       [7.17516906e-03],\n",
              "       [7.52944259e-03],\n",
              "       [7.86417583e-03],\n",
              "       [8.21908638e-03],\n",
              "       [8.66566289e-03],\n",
              "       [9.04772818e-03],\n",
              "       [9.43085896e-03],\n",
              "       [9.85682951e-03],\n",
              "       [1.02654562e-02],\n",
              "       [1.07535259e-02],\n",
              "       [1.12109240e-02],\n",
              "       [1.17231264e-02],\n",
              "       [1.21481509e-02],\n",
              "       [1.27614329e-02],\n",
              "       [1.33718038e-02],\n",
              "       [1.39628927e-02],\n",
              "       [1.45817672e-02],\n",
              "       [1.51811196e-02],\n",
              "       [1.58271367e-02],\n",
              "       [1.63843924e-02],\n",
              "       [1.69190903e-02],\n",
              "       [1.75973844e-02],\n",
              "       [1.85827997e-02],\n",
              "       [1.97506400e-02],\n",
              "       [2.07579465e-02],\n",
              "       [2.19188660e-02],\n",
              "       [2.29528207e-02],\n",
              "       [2.50158511e-02],\n",
              "       [2.69458894e-02],\n",
              "       [2.88883653e-02],\n",
              "       [3.10160865e-02],\n",
              "       [3.34812527e-02],\n",
              "       [3.58028876e-02],\n",
              "       [3.79871332e-02],\n",
              "       [4.08315346e-02],\n",
              "       [4.33472368e-02],\n",
              "       [4.59601482e-02],\n",
              "       [4.86242938e-02],\n",
              "       [5.13205110e-02],\n",
              "       [5.36201431e-02],\n",
              "       [5.58224275e-02],\n",
              "       [5.78284734e-02],\n",
              "       [6.04865621e-02],\n",
              "       [6.33747203e-02],\n",
              "       [6.59729681e-02],\n",
              "       [6.89440176e-02],\n",
              "       [7.17862119e-02],\n",
              "       [7.45604855e-02],\n",
              "       [7.68941835e-02],\n",
              "       [7.89512512e-02],\n",
              "       [8.07206513e-02],\n",
              "       [8.25805811e-02],\n",
              "       [8.45429170e-02],\n",
              "       [8.63896763e-02],\n",
              "       [8.82044671e-02],\n",
              "       [9.07555679e-02],\n",
              "       [9.32986663e-02],\n",
              "       [9.63477793e-02],\n",
              "       [9.94614188e-02],\n",
              "       [1.02808195e-01],\n",
              "       [1.06043848e-01],\n",
              "       [1.09003450e-01],\n",
              "       [1.11895423e-01],\n",
              "       [1.14870368e-01],\n",
              "       [1.18006485e-01],\n",
              "       [1.21258680e-01],\n",
              "       [1.24420613e-01],\n",
              "       [1.27783442e-01],\n",
              "       [1.31368528e-01],\n",
              "       [1.34761484e-01],\n",
              "       [1.38282768e-01],\n",
              "       [1.41840197e-01],\n",
              "       [1.45567197e-01],\n",
              "       [1.49152143e-01],\n",
              "       [1.52264615e-01],\n",
              "       [1.54944427e-01],\n",
              "       [1.57831581e-01],\n",
              "       [1.61059389e-01],\n",
              "       [1.64098077e-01],\n",
              "       [1.67038438e-01],\n",
              "       [1.70224550e-01],\n",
              "       [1.73474123e-01],\n",
              "       [1.76461522e-01],\n",
              "       [1.79531191e-01],\n",
              "       [1.82814254e-01],\n",
              "       [1.85626874e-01],\n",
              "       [1.88311471e-01],\n",
              "       [1.91035615e-01],\n",
              "       [1.93457224e-01],\n",
              "       [1.95527664e-01],\n",
              "       [1.97850669e-01],\n",
              "       [1.99944136e-01],\n",
              "       [2.02060352e-01],\n",
              "       [2.04039248e-01],\n",
              "       [2.05836249e-01],\n",
              "       [2.07385091e-01],\n",
              "       [2.08712514e-01],\n",
              "       [2.10095841e-01],\n",
              "       [2.11650426e-01],\n",
              "       [2.12987750e-01],\n",
              "       [2.14434611e-01],\n",
              "       [2.15757711e-01],\n",
              "       [2.16981441e-01],\n",
              "       [2.17930236e-01],\n",
              "       [2.19092654e-01],\n",
              "       [2.20056386e-01],\n",
              "       [2.21089986e-01],\n",
              "       [2.22079645e-01],\n",
              "       [2.23021456e-01],\n",
              "       [2.23920431e-01],\n",
              "       [2.24624373e-01],\n",
              "       [2.25302709e-01],\n",
              "       [2.26050637e-01],\n",
              "       [2.26908906e-01],\n",
              "       [2.27692729e-01],\n",
              "       [2.28460764e-01],\n",
              "       [2.29275066e-01],\n",
              "       [2.29965082e-01],\n",
              "       [2.30601675e-01],\n",
              "       [2.31203794e-01],\n",
              "       [2.31861896e-01],\n",
              "       [2.32513996e-01],\n",
              "       [2.33189157e-01],\n",
              "       [2.33831033e-01],\n",
              "       [2.34386658e-01],\n",
              "       [2.34917431e-01],\n",
              "       [2.35412489e-01],\n",
              "       [2.35855210e-01],\n",
              "       [2.36288642e-01],\n",
              "       [2.36673726e-01],\n",
              "       [2.37034092e-01],\n",
              "       [2.37432385e-01],\n",
              "       [2.37822632e-01],\n",
              "       [2.38250726e-01],\n",
              "       [2.38722607e-01],\n",
              "       [2.39266793e-01],\n",
              "       [2.39808834e-01],\n",
              "       [2.40465756e-01],\n",
              "       [2.41086022e-01],\n",
              "       [2.41726322e-01],\n",
              "       [2.42379722e-01],\n",
              "       [2.42974790e-01],\n",
              "       [2.43712739e-01],\n",
              "       [2.44331402e-01],\n",
              "       [2.44978141e-01],\n",
              "       [2.45512298e-01],\n",
              "       [2.46089700e-01],\n",
              "       [2.46659253e-01],\n",
              "       [2.47200302e-01],\n",
              "       [2.47702125e-01],\n",
              "       [2.48162674e-01],\n",
              "       [2.48690851e-01],\n",
              "       [2.49110108e-01],\n",
              "       [2.49505745e-01],\n",
              "       [2.49908771e-01],\n",
              "       [2.50305562e-01],\n",
              "       [2.50700506e-01],\n",
              "       [2.51086479e-01],\n",
              "       [2.51477940e-01],\n",
              "       [2.51810191e-01],\n",
              "       [2.52181038e-01],\n",
              "       [2.51999627e-01],\n",
              "       [2.52025889e-01],\n",
              "       [2.52184727e-01],\n",
              "       [2.52349328e-01],\n",
              "       [2.52524458e-01],\n",
              "       [2.52789664e-01],\n",
              "       [2.53123493e-01],\n",
              "       [2.53438967e-01],\n",
              "       [2.53769182e-01],\n",
              "       [2.54077009e-01],\n",
              "       [2.54363101e-01],\n",
              "       [2.54665150e-01],\n",
              "       [2.54931099e-01],\n",
              "       [2.55184718e-01],\n",
              "       [2.55486971e-01],\n",
              "       [2.55815738e-01],\n",
              "       [2.56184580e-01],\n",
              "       [2.56544132e-01],\n",
              "       [2.56919406e-01],\n",
              "       [2.57376619e-01],\n",
              "       [2.57773829e-01],\n",
              "       [2.58182843e-01],\n",
              "       [2.58568757e-01],\n",
              "       [2.58965582e-01],\n",
              "       [2.59336213e-01],\n",
              "       [2.59726846e-01],\n",
              "       [2.60129325e-01],\n",
              "       [2.60497904e-01],\n",
              "       [2.60922981e-01],\n",
              "       [2.61375753e-01],\n",
              "       [2.61767088e-01],\n",
              "       [2.62167195e-01],\n",
              "       [2.62563510e-01],\n",
              "       [2.62871559e-01],\n",
              "       [2.63179634e-01],\n",
              "       [2.63480863e-01],\n",
              "       [2.63763844e-01],\n",
              "       [2.64067610e-01],\n",
              "       [2.64379245e-01],\n",
              "       [2.64679194e-01],\n",
              "       [2.64918648e-01],\n",
              "       [2.65237437e-01],\n",
              "       [2.65546567e-01],\n",
              "       [2.65866595e-01],\n",
              "       [2.66139448e-01],\n",
              "       [2.66474399e-01],\n",
              "       [2.66781828e-01],\n",
              "       [2.67032203e-01],\n",
              "       [2.67234678e-01],\n",
              "       [2.67473379e-01],\n",
              "       [2.67693120e-01],\n",
              "       [2.67921296e-01],\n",
              "       [2.68185591e-01],\n",
              "       [2.68481798e-01],\n",
              "       [2.68721569e-01],\n",
              "       [2.68980148e-01],\n",
              "       [2.69283426e-01],\n",
              "       [2.69520078e-01],\n",
              "       [2.69785560e-01],\n",
              "       [2.70080497e-01],\n",
              "       [2.70374994e-01],\n",
              "       [2.70622803e-01],\n",
              "       [2.70907837e-01],\n",
              "       [2.71244772e-01],\n",
              "       [2.71644061e-01],\n",
              "       [2.72109177e-01],\n",
              "       [2.72575806e-01],\n",
              "       [2.73125868e-01],\n",
              "       [2.73564993e-01],\n",
              "       [2.74052801e-01],\n",
              "       [2.74589977e-01],\n",
              "       [2.75183298e-01],\n",
              "       [2.75771213e-01],\n",
              "       [2.76343611e-01],\n",
              "       [2.76956213e-01],\n",
              "       [2.77446742e-01],\n",
              "       [2.78021072e-01],\n",
              "       [2.78682558e-01],\n",
              "       [2.79353707e-01],\n",
              "       [2.80105406e-01],\n",
              "       [2.80862268e-01],\n",
              "       [2.81718037e-01],\n",
              "       [2.82496731e-01],\n",
              "       [2.83301612e-01],\n",
              "       [2.84229631e-01],\n",
              "       [2.85152764e-01],\n",
              "       [2.86274229e-01],\n",
              "       [2.87370552e-01],\n",
              "       [2.88545893e-01],\n",
              "       [2.89783649e-01],\n",
              "       [2.91162460e-01],\n",
              "       [2.92878982e-01],\n",
              "       [2.94803186e-01],\n",
              "       [2.96759847e-01],\n",
              "       [2.98928775e-01],\n",
              "       [3.01201978e-01],\n",
              "       [3.03523189e-01],\n",
              "       [3.06118868e-01],\n",
              "       [3.08863333e-01],\n",
              "       [3.11891260e-01],\n",
              "       [3.15159690e-01],\n",
              "       [3.18485976e-01],\n",
              "       [3.22122312e-01],\n",
              "       [3.26008188e-01],\n",
              "       [3.29872550e-01],\n",
              "       [3.34335340e-01],\n",
              "       [3.38965216e-01],\n",
              "       [3.43838211e-01],\n",
              "       [3.49027724e-01],\n",
              "       [3.54788531e-01],\n",
              "       [3.59956193e-01],\n",
              "       [3.65815497e-01],\n",
              "       [3.71631228e-01],\n",
              "       [3.77620549e-01],\n",
              "       [3.83958418e-01],\n",
              "       [3.90786094e-01],\n",
              "       [3.98648130e-01],\n",
              "       [4.06431831e-01],\n",
              "       [4.15056510e-01],\n",
              "       [4.23737362e-01],\n",
              "       [4.33067445e-01],\n",
              "       [4.43071444e-01],\n",
              "       [4.53012263e-01],\n",
              "       [4.63284444e-01],\n",
              "       [4.73127929e-01],\n",
              "       [4.83366199e-01],\n",
              "       [4.93818884e-01],\n",
              "       [5.04440487e-01],\n",
              "       [5.15383440e-01],\n",
              "       [5.25887664e-01],\n",
              "       [5.37000192e-01],\n",
              "       [5.46868309e-01],\n",
              "       [5.57898250e-01],\n",
              "       [5.68915211e-01],\n",
              "       [5.80742940e-01],\n",
              "       [5.93126791e-01],\n",
              "       [6.05756009e-01],\n",
              "       [6.17256834e-01],\n",
              "       [6.27897780e-01],\n",
              "       [6.38044154e-01],\n",
              "       [6.48178906e-01],\n",
              "       [6.58554388e-01],\n",
              "       [6.68424366e-01],\n",
              "       [6.78187544e-01],\n",
              "       [6.87604756e-01],\n",
              "       [6.96283942e-01],\n",
              "       [7.05472623e-01],\n",
              "       [7.14862573e-01],\n",
              "       [7.23726458e-01],\n",
              "       [7.32514234e-01],\n",
              "       [7.40829343e-01],\n",
              "       [7.49116006e-01],\n",
              "       [7.57102441e-01],\n",
              "       [7.65242509e-01],\n",
              "       [7.73022228e-01],\n",
              "       [7.81422184e-01],\n",
              "       [7.94099019e-01],\n",
              "       [8.04446298e-01],\n",
              "       [8.13958067e-01],\n",
              "       [8.22223326e-01],\n",
              "       [8.29976758e-01],\n",
              "       [8.35797292e-01],\n",
              "       [8.41360957e-01],\n",
              "       [8.46341127e-01],\n",
              "       [8.50900568e-01],\n",
              "       [8.55054389e-01],\n",
              "       [8.59298879e-01],\n",
              "       [8.63305436e-01],\n",
              "       [8.67111362e-01],\n",
              "       [8.70867566e-01],\n",
              "       [8.74386100e-01],\n",
              "       [8.78682914e-01],\n",
              "       [8.82479494e-01],\n",
              "       [8.85618451e-01],\n",
              "       [8.88581828e-01],\n",
              "       [8.91308670e-01],\n",
              "       [8.93619106e-01],\n",
              "       [8.95688880e-01],\n",
              "       [8.97541284e-01],\n",
              "       [8.99275892e-01],\n",
              "       [9.00781415e-01],\n",
              "       [9.02435229e-01],\n",
              "       [9.04020863e-01],\n",
              "       [9.05509615e-01],\n",
              "       [9.07207028e-01],\n",
              "       [9.08946049e-01],\n",
              "       [9.10517744e-01],\n",
              "       [9.11968361e-01],\n",
              "       [9.13344944e-01],\n",
              "       [9.14823216e-01],\n",
              "       [9.16220044e-01],\n",
              "       [9.17599822e-01],\n",
              "       [9.18875746e-01],\n",
              "       [9.20056931e-01],\n",
              "       [9.21022528e-01],\n",
              "       [9.21993843e-01],\n",
              "       [9.22847389e-01],\n",
              "       [9.23693801e-01],\n",
              "       [9.24436378e-01],\n",
              "       [9.25224690e-01],\n",
              "       [9.25984554e-01],\n",
              "       [9.26724011e-01],\n",
              "       [9.27552983e-01],\n",
              "       [9.28439635e-01],\n",
              "       [9.29360011e-01],\n",
              "       [9.30279502e-01],\n",
              "       [9.31089837e-01],\n",
              "       [9.32037058e-01],\n",
              "       [9.33322385e-01],\n",
              "       [9.34743833e-01],\n",
              "       [9.35996528e-01],\n",
              "       [9.37237312e-01],\n",
              "       [9.38337146e-01],\n",
              "       [9.39422105e-01],\n",
              "       [9.40411870e-01],\n",
              "       [9.41358683e-01],\n",
              "       [9.42445268e-01],\n",
              "       [9.43585955e-01],\n",
              "       [9.44548208e-01],\n",
              "       [9.45432374e-01],\n",
              "       [9.46310780e-01],\n",
              "       [9.47273186e-01],\n",
              "       [9.48084305e-01],\n",
              "       [9.49097065e-01],\n",
              "       [9.50145307e-01],\n",
              "       [9.52170514e-01],\n",
              "       [9.53660354e-01],\n",
              "       [9.55487973e-01],\n",
              "       [9.57276148e-01],\n",
              "       [9.58839811e-01],\n",
              "       [9.60125052e-01],\n",
              "       [9.61494579e-01],\n",
              "       [9.62432319e-01],\n",
              "       [9.63250375e-01],\n",
              "       [9.64057037e-01],\n",
              "       [9.64809817e-01],\n",
              "       [9.65611026e-01],\n",
              "       [9.66996461e-01],\n",
              "       [9.68355359e-01],\n",
              "       [9.69863851e-01],\n",
              "       [9.71332980e-01],\n",
              "       [9.72795326e-01],\n",
              "       [9.74029465e-01],\n",
              "       [9.75129535e-01],\n",
              "       [9.76118554e-01],\n",
              "       [9.77042337e-01],\n",
              "       [9.78021490e-01],\n",
              "       [9.78902710e-01],\n",
              "       [9.79836329e-01],\n",
              "       [9.80724836e-01],\n",
              "       [9.81548782e-01],\n",
              "       [9.82376707e-01],\n",
              "       [9.83271629e-01],\n",
              "       [9.84132311e-01],\n",
              "       [9.84936628e-01],\n",
              "       [9.85776281e-01],\n",
              "       [9.86601054e-01],\n",
              "       [9.87294431e-01],\n",
              "       [9.88115483e-01],\n",
              "       [9.88937856e-01],\n",
              "       [9.89720235e-01],\n",
              "       [9.90474885e-01],\n",
              "       [9.91253342e-01],\n",
              "       [9.92025937e-01],\n",
              "       [9.92706636e-01],\n",
              "       [9.93441743e-01],\n",
              "       [9.94198417e-01],\n",
              "       [9.92573244e-01],\n",
              "       [9.91854088e-01],\n",
              "       [9.91378937e-01],\n",
              "       [9.91075764e-01],\n",
              "       [9.90704048e-01],\n",
              "       [9.91061553e-01],\n",
              "       [9.91689618e-01],\n",
              "       [9.92473283e-01],\n",
              "       [9.93211814e-01],\n",
              "       [9.93932996e-01],\n",
              "       [9.94696811e-01],\n",
              "       [9.95301212e-01],\n",
              "       [9.95962128e-01],\n",
              "       [9.96616942e-01],\n",
              "       [9.97297020e-01],\n",
              "       [9.97928551e-01],\n",
              "       [9.98658404e-01],\n",
              "       [9.99237241e-01],\n",
              "       [9.99528745e-01],\n",
              "       [1.00000000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_PQDtuTntcE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-12T14:12:06.378833Z",
          "iopub.status.busy": "2021-06-12T14:12:06.378417Z",
          "iopub.status.idle": "2021-06-12T14:12:06.388852Z",
          "shell.execute_reply": "2021-06-12T14:12:06.387828Z",
          "shell.execute_reply.started": "2021-06-12T14:12:06.378779Z"
        },
        "id": "KzFTOxc3ntcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e489f0e9-e31e-4615-a621-fc6880edd33b"
      },
      "source": [
        "data_scaled.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(521, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmkDMdDTntcF"
      },
      "source": [
        "<a id=\"25\"></a> <br>\n",
        "## Creating LSTM input data\n",
        "\n",
        "In order to use LSTM, our input and output data should have a specific shape. It was a bit complicated for me when I was first introduced to LSTM, but I found [this](https://medium.com/@shivajbd/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e) comprehensive article to fully understand what was going on. In a nutshell, the input and output data in an LSTM model is a three-dimensional array where the first dimension represents **the number of samples (or batch size)** like the number of rows of data in a two-dimensional setting, the second dimension stands for **time steps** which indicates the amount of time that we want to go back through time, and the third dimension shows **the number of features (or input dimension)** that we want to include in the model for every element in our batch. So, it is like [number_of_samples, time_steps, input_dim]. The below image is retrieved from the mentioned article and could be a good illustration of the LSTM input and output data shape.\n",
        "![](https://miro.medium.com/max/665/1*AQKRJsRdWx2HZ85H1yWoKw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNxoXjhhntcF"
      },
      "source": [
        "Sometimes we prefer to choose the validation set by ourselves. In this case, in the below code, you can pass a value in the range of (0, 1) for the validation_split_percentage when you are calling the create_data function, and use the below code to fit the model:\n",
        "\n",
        "- model.fit(train_X, train_y, validation_data=(val_x, val_y))  # manually splitting\n",
        "\n",
        "In my case, I rathered to use the built-in parameter (validation_split) in the fit method of the Keras library.  It considers a fraction of the training data as the validation set to evaluate loss and metrics at the end of each epoch as follow:\n",
        "\n",
        "- model.fit(train_X, train_y, validation_split=0.3)  # automatically splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxsFryxRntcG"
      },
      "source": [
        "Here, at each point of the time, we will consider price and volume as our attributes in input_dim, and 25 days as our time_steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-12T14:12:06.390648Z",
          "iopub.status.busy": "2021-06-12T14:12:06.390258Z",
          "iopub.status.idle": "2021-06-12T14:12:06.402553Z",
          "shell.execute_reply": "2021-06-12T14:12:06.401666Z",
          "shell.execute_reply.started": "2021-06-12T14:12:06.390608Z"
        },
        "id": "pN-8hEzxntcG"
      },
      "source": [
        "# Creating a data structure (it does not work when you have only one feature)\n",
        "def create_data(df, n_future, n_past, train_test_split_percentage, validation_split_percentage):\n",
        "    n_feature = df.shape[1]\n",
        "    x_data, y_data = [], []\n",
        "    \n",
        "    for i in range(n_past, len(df) - n_future + 1):\n",
        "        x_data.append(df[i - n_past:i, 0:n_feature])\n",
        "        y_data.append(df[i + n_future - 1:i + n_future, 0])\n",
        "    \n",
        "    split_training_test_starting_point = int(round(train_test_split_percentage*len(x_data)))\n",
        "    split_train_validation_starting_point = int(round(split_training_test_starting_point*(1-validation_split_percentage)))\n",
        "    \n",
        "    x_train = x_data[:split_train_validation_starting_point]\n",
        "    y_train = y_data[:split_train_validation_starting_point]\n",
        "    \n",
        "    # if you want to choose the validation set by yourself, uncomment the below code.\n",
        "    x_val = x_data[split_train_validation_starting_point:split_training_test_starting_point]\n",
        "    y_val =  x_data[split_train_validation_starting_point:split_training_test_starting_point]                                             \n",
        "    \n",
        "    x_test = x_data[split_training_test_starting_point:]\n",
        "    y_test = y_data[split_training_test_starting_point:]\n",
        "    \n",
        "    return np.array(x_train), np.array(x_test), np.array(x_val), np.array(y_train), np.array(y_test), np.array(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-12T14:12:06.404197Z",
          "iopub.status.busy": "2021-06-12T14:12:06.403866Z",
          "iopub.status.idle": "2021-06-12T14:12:06.423225Z",
          "shell.execute_reply": "2021-06-12T14:12:06.422397Z",
          "shell.execute_reply.started": "2021-06-12T14:12:06.404161Z"
        },
        "id": "VEhCZ5N5ntcH"
      },
      "source": [
        "# Number of days you want to predict into the future\n",
        "# Number of past days you want to use to predict the future\n",
        "\n",
        "X_train, X_test, X_val, y_train, y_test, y_val = create_data(data_scaled, n_future=1, n_past=25, train_test_split_percentage=0.8,\n",
        "                                               validation_split_percentage = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-12T14:12:06.425058Z",
          "iopub.status.busy": "2021-06-12T14:12:06.424527Z",
          "iopub.status.idle": "2021-06-12T14:12:06.431459Z",
          "shell.execute_reply": "2021-06-12T14:12:06.430664Z",
          "shell.execute_reply.started": "2021-06-12T14:12:06.425026Z"
        },
        "id": "4blu31PbntcH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8d6ce3e-7807-4533-f9cd-42900c360c66"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(397, 25, 1)\n",
            "(99, 25, 1)\n",
            "(397, 1)\n",
            "(99, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hane31P5ntcH"
      },
      "source": [
        "<a id=\"3\"></a> <br>\n",
        "## Train LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-12T14:12:06.433208Z",
          "iopub.status.busy": "2021-06-12T14:12:06.432876Z",
          "iopub.status.idle": "2021-06-12T14:12:06.443758Z",
          "shell.execute_reply": "2021-06-12T14:12:06.442733Z",
          "shell.execute_reply.started": "2021-06-12T14:12:06.433181Z"
        },
        "id": "q3BoLI_fntcH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "95048ae9-2af9-4efa-9046-1ee8c63c516c"
      },
      "source": [
        "# ------------------LSTM-----------------------\n",
        "'''''\n",
        "regressor = Sequential()\n",
        "regressor.add(LSTM(units=256, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "regressor.add(LSTM(units=256, return_sequences=False))\n",
        "regressor.add(Dropout(0.2))\n",
        "regressor.add(Dense(units=1, activation='linear'))\n",
        "regressor.compile(optimizer='adam', loss='mse')\n",
        "#regressor.fit(X_train, y_train, epochs=100, batch_size=64)\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "#mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "# fit model\n",
        "history = regressor.fit(X_train, y_train, validation_split=0.3, epochs=1000, batch_size=64, callbacks=[es])\n",
        "'''''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"''\\nregressor = Sequential()\\nregressor.add(LSTM(units=256, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\\nregressor.add(Dropout(0.2))\\n\\nregressor.add(LSTM(units=256, return_sequences=False))\\nregressor.add(Dropout(0.2))\\nregressor.add(Dense(units=1, activation='linear'))\\nregressor.compile(optimizer='adam', loss='mse')\\n#regressor.fit(X_train, y_train, epochs=100, batch_size=64)\\n\\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\\n#mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\\n# fit model\\nhistory = regressor.fit(X_train, y_train, validation_split=0.3, epochs=1000, batch_size=64, callbacks=[es])\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0ul8e8_ntcI"
      },
      "source": [
        "<a id=\"4\"></a> <br>\n",
        "## Hyperparameter Tunning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGtj_jJWntcI"
      },
      "source": [
        "\n",
        "We have two important terms in machine learning referred to as **model parameter** and **model hyperparamer**. So, first of all, what is a hyperparameter, and what is a parameter? Based on [here](https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models),\n",
        "\n",
        "- A model **model hyperparameter** is a configuration that is external to the model and whose value cannot be estimated from the data and a **model parameter** is a configuration variable that is internal to the model and whose value can be estimated from the given data.\n",
        "\n",
        "In the other words, a hyperparameter is used to construct the structure of the model and cannot be learned from the data and its value is set before the learning process begins. Therefore, hyperparameters are like the settings of an algorithm that can be adjusted to optimize performance and prevent overfitting. This is exactly what we do in the hyperparameter tuning. We try to choose a set of optimal hyperparameters for a learning algorithm to enhance the performance of the model. There are two frequently used methods to perform hyperparameter tunning called 1)Grid Search and 2)Random Search. In this notebook, I have used the former one because of its simplicity to implement and at the same time, its powerful performance. More information on both of the methods can be found in [here](https://blog.usejournal.com/a-comparison-of-grid-search-and-randomized-search-using-scikit-learn-29823179bc85)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD28RbdzntcI"
      },
      "source": [
        "<a id=\"42\"></a> <br>\n",
        "## Grid Search\n",
        "\n",
        "Grid search is a traditional method to perform hyperparameter tunning. It basically works by defining a subset of candidate values for each hyperparameter, and training all the possible combination of the hyperparameters. Then, each possible fitted model is evaluated on a validation set, and the best configuration of the hyperparameter will be choosed at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiN95-J2ntcI"
      },
      "source": [
        "<a id=\"43\"></a> <br>\n",
        "## Early Stopping and Callback\n",
        "\n",
        "Since in the Grid Search, we have to train an LSTM model for each combination, it may take so much time to fit all the models and choose the best combination of the hyperparameters. One of the ways that we can prevent this from happening is through using Early Stopping and Callbacks. The idea here is to track a measure (like validation loss) and whenever a stopping criterion (like no improvement in the monitored measure value in successive steps, reaching a pre-specified limit for that measure, or a pre-specified increment in that measure) is satisfied, we can stop the training process. The measure that we are using here is validation loss since the validation set is not used in the training process. [This](https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/) article is one of the best articles that I've read about using Early Stopping to halt the training of a model at the right time. In the below, I copied and pasted the parts that I found important:\n",
        "\n",
        "> 1. **es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)** ****-->**** Often, the first sign of no further improvement may not be the best time to stop training. This is because the model may coast into a plateau of no improvement or even get slightly worse before getting much better. We can account for this by adding a delay to the trigger in terms of the number of epochs on which we would like to see no improvement. This can be done by setting the “patience” argument.\n",
        "    \n",
        "> 2. **es = EarlyStopping(monitor='val_accuracy', mode='max', min_delta=1)** **-->** By default, any change in the performance measure, no matter how fractional, will be considered an improvement. You may want to consider an improvement that is a specific increment, such as 1 unit for mean squared error or 1% for accuracy. This can be specified via the “min_delta” argument.\n",
        "\n",
        "> 3. **es = EarlyStopping(monitor='val_loss', mode='min', baseline=0.4)** **-->**  Finally, it may be desirable to only stop training if performance stays above or below a given threshold or baseline. For example, if you have familiarity with the training of the model (e.g. learning curves) and know that once a validation loss of a given value is achieved that there is no point in continuing training. This can be specified by setting the “baseline” argument.\n",
        "    \n",
        "> 4. **mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1)** **-->** The EarlyStopping callback will stop training once triggered, but the model at the end of training may not be the model with best performance on the validation dataset. An additional callback is required that will save the best model observed during training for later use. This is the ModelCheckpoint callback."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-12T14:12:06.445745Z",
          "iopub.status.busy": "2021-06-12T14:12:06.445314Z",
          "iopub.status.idle": "2021-06-12T14:12:06.469099Z",
          "shell.execute_reply": "2021-06-12T14:12:06.467977Z",
          "shell.execute_reply.started": "2021-06-12T14:12:06.445702Z"
        },
        "id": "Y0xcYatentcJ"
      },
      "source": [
        "# detect and init the TPU\n",
        "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "# tf.config.experimental_connect_to_cluster(tpu)\n",
        "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\n",
        "# instantiate a distribution strategy\n",
        "# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "\n",
        "def LSTM_HyperParameter_Tuning(config, x_train, y_train, x_test, y_test):\n",
        "    \n",
        "    first_additional_layer, second_additional_layer, third_additional_layer, n_neurons, n_batch_size, dropout = config\n",
        "    possible_combinations = list(itertools.product(first_additional_layer, second_additional_layer, third_additional_layer,\n",
        "                                                  n_neurons, n_batch_size, dropout))\n",
        "    \n",
        "    print(possible_combinations)\n",
        "    print('\\n')\n",
        "    print(print(np.asarray(possible_combinations).shape))\n",
        "    hist = []\n",
        "    \n",
        "    for i in range(0, len(possible_combinations)):\n",
        "        \n",
        "        print(f'{i}th combination: \\n')\n",
        "        print('--------------------------------------------------------------------')\n",
        "        \n",
        "        first_additional_layer, second_additional_layer, third_additional_layer, n_neurons, n_batch_size, dropout = possible_combinations[i]\n",
        "        \n",
        "        # instantiating the model in the strategy scope creates the model on the TPU\n",
        "        #with tpu_strategy.scope():\n",
        "        regressor = Sequential()\n",
        "        regressor.add(LSTM(units=n_neurons, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
        "        regressor.add(Dropout(dropout))\n",
        "\n",
        "        if first_additional_layer:\n",
        "            regressor.add(LSTM(units=n_neurons, return_sequences=True))\n",
        "            regressor.add(Dropout(dropout))\n",
        "\n",
        "        if second_additional_layer:\n",
        "            regressor.add(LSTM(units=n_neurons, return_sequences=True))\n",
        "            regressor.add(Dropout(dropout))\n",
        "\n",
        "        if third_additional_layer:\n",
        "            regressor.add(GRU(units=n_neurons, return_sequences=True))\n",
        "            regressor.add(Dropout(dropout))\n",
        "\n",
        "        regressor.add(LSTM(units=n_neurons, return_sequences=False))\n",
        "        regressor.add(Dropout(dropout))\n",
        "        regressor.add(Dense(units=1, activation='linear'))\n",
        "        regressor.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
        "        '''''\n",
        "        From the mentioned article above --> If a validation dataset is specified to the fit() function via the validation_data or v\n",
        "        alidation_split arguments,then the loss on the validation dataset will be made available via the name “val_loss.”\n",
        "        '''''\n",
        "\n",
        "        file_path = 'best_model.h5'\n",
        "\n",
        "        mc = ModelCheckpoint(file_path, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "\n",
        "        '''''\n",
        "        cb = Callback(...)  # First, callbacks must be instantiated.\n",
        "        cb_list = [cb, ...]  # Then, one or more callbacks that you intend to use must be added to a Python list.\n",
        "        model.fit(..., callbacks=cb_list)  # Finally, the list of callbacks is provided to the callback argument when fitting the model.\n",
        "        '''''\n",
        "\n",
        "        regressor.fit(x_train, y_train, validation_split=0.3, epochs=1000, batch_size=n_batch_size, callbacks=[es, mc], verbose=0)\n",
        "\n",
        "        # load the best model\n",
        "        # regressor = load_model('best_model.h5')\n",
        "\n",
        "        train_accuracy = regressor.evaluate(x_train, y_train, verbose=0)\n",
        "        test_accuracy = regressor.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "        hist.append(list((first_additional_layer, second_additional_layer, third_additional_layer, n_neurons, n_batch_size, dropout,\n",
        "                          train_accuracy, test_accuracy)))\n",
        "\n",
        "        print(f'{str(i)}-th combination = {possible_combinations[i]} \\n train accuracy: {train_accuracy} and test accuracy: {test_accuracy}')\n",
        "        \n",
        "        print('--------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------')\n",
        "        \n",
        "    return hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-12T14:12:06.470755Z",
          "iopub.status.busy": "2021-06-12T14:12:06.470272Z"
        },
        "id": "zyW9cCLpntcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "416b5347-20d1-44da-c8d9-8ac06daf568e"
      },
      "source": [
        "config = [[False, True], [False, True], [False, True], [128, 256], [128, 64], [0.1, 0.2]]   \n",
        "\n",
        "# list of lists --> [[first_additional_layer], [second_additional_layer], [third_additional_layer], [n_neurons], [n_batch_size], [dropout]]\n",
        "\n",
        "hist = LSTM_HyperParameter_Tuning(config, X_train, y_train, X_test, y_test)  # change x_train shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(False, False, False, 128, 128, 0.1), (False, False, False, 128, 128, 0.2), (False, False, False, 128, 64, 0.1), (False, False, False, 128, 64, 0.2), (False, False, False, 256, 128, 0.1), (False, False, False, 256, 128, 0.2), (False, False, False, 256, 64, 0.1), (False, False, False, 256, 64, 0.2), (False, False, True, 128, 128, 0.1), (False, False, True, 128, 128, 0.2), (False, False, True, 128, 64, 0.1), (False, False, True, 128, 64, 0.2), (False, False, True, 256, 128, 0.1), (False, False, True, 256, 128, 0.2), (False, False, True, 256, 64, 0.1), (False, False, True, 256, 64, 0.2), (False, True, False, 128, 128, 0.1), (False, True, False, 128, 128, 0.2), (False, True, False, 128, 64, 0.1), (False, True, False, 128, 64, 0.2), (False, True, False, 256, 128, 0.1), (False, True, False, 256, 128, 0.2), (False, True, False, 256, 64, 0.1), (False, True, False, 256, 64, 0.2), (False, True, True, 128, 128, 0.1), (False, True, True, 128, 128, 0.2), (False, True, True, 128, 64, 0.1), (False, True, True, 128, 64, 0.2), (False, True, True, 256, 128, 0.1), (False, True, True, 256, 128, 0.2), (False, True, True, 256, 64, 0.1), (False, True, True, 256, 64, 0.2), (True, False, False, 128, 128, 0.1), (True, False, False, 128, 128, 0.2), (True, False, False, 128, 64, 0.1), (True, False, False, 128, 64, 0.2), (True, False, False, 256, 128, 0.1), (True, False, False, 256, 128, 0.2), (True, False, False, 256, 64, 0.1), (True, False, False, 256, 64, 0.2), (True, False, True, 128, 128, 0.1), (True, False, True, 128, 128, 0.2), (True, False, True, 128, 64, 0.1), (True, False, True, 128, 64, 0.2), (True, False, True, 256, 128, 0.1), (True, False, True, 256, 128, 0.2), (True, False, True, 256, 64, 0.1), (True, False, True, 256, 64, 0.2), (True, True, False, 128, 128, 0.1), (True, True, False, 128, 128, 0.2), (True, True, False, 128, 64, 0.1), (True, True, False, 128, 64, 0.2), (True, True, False, 256, 128, 0.1), (True, True, False, 256, 128, 0.2), (True, True, False, 256, 64, 0.1), (True, True, False, 256, 64, 0.2), (True, True, True, 128, 128, 0.1), (True, True, True, 128, 128, 0.2), (True, True, True, 128, 64, 0.1), (True, True, True, 128, 64, 0.2), (True, True, True, 256, 128, 0.1), (True, True, True, 256, 128, 0.2), (True, True, True, 256, 64, 0.1), (True, True, True, 256, 64, 0.2)]\n",
            "\n",
            "\n",
            "(64, 6)\n",
            "None\n",
            "0th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00243, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00243\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00243\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00243\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00243\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00243 to 0.00077, saving model to best_model.h5\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00077\n",
            "Epoch 00026: early stopping\n",
            "0-th combination = (False, False, False, 128, 128, 0.1) \n",
            " train accuracy: 0.0017109981272369623 and test accuracy: 0.0099030127748847\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "1th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.03340, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.03340 to 0.00356, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00356\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00356\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00356\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00356 to 0.00106, saving model to best_model.h5\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00106\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00106\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00106\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00106\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.00106 to 0.00095, saving model to best_model.h5\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00095\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00095\n",
            "Epoch 00031: early stopping\n",
            "1-th combination = (False, False, False, 128, 128, 0.2) \n",
            " train accuracy: 0.0020680450834333897 and test accuracy: 0.015302449464797974\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "2th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00214, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00214\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00214\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00214\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00214\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00214 to 0.00069, saving model to best_model.h5\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00069\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00069\n",
            "Epoch 00026: early stopping\n",
            "2-th combination = (False, False, False, 128, 64, 0.1) \n",
            " train accuracy: 0.0018139713210985065 and test accuracy: 0.013862763531506062\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "3th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00231, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00231\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00231\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00231 to 0.00077, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00077\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00077 to 0.00066, saving model to best_model.h5\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00066\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00066\n",
            "Epoch 00027: early stopping\n",
            "3-th combination = (False, False, False, 128, 64, 0.2) \n",
            " train accuracy: 0.002349406946450472 and test accuracy: 0.018229834735393524\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "4th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00142, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00142 to 0.00102, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00102\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00102\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00102\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00102 to 0.00049, saving model to best_model.h5\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00049\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00049\n",
            "Epoch 00028: early stopping\n",
            "4-th combination = (False, False, False, 256, 128, 0.1) \n",
            " train accuracy: 0.001399574219249189 and test accuracy: 0.008858934976160526\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "5th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00151, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00151\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00151\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00151 to 0.00092, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00092\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00092\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00092\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00092 to 0.00053, saving model to best_model.h5\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00053\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00053\n",
            "Epoch 00028: early stopping\n",
            "5-th combination = (False, False, False, 256, 128, 0.2) \n",
            " train accuracy: 0.0012471334775909781 and test accuracy: 0.007802137173712254\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "6th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.04436, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.04436 to 0.00105, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00105 to 0.00065, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00065\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00065\n",
            "Epoch 00024: early stopping\n",
            "6-th combination = (False, False, False, 256, 64, 0.1) \n",
            " train accuracy: 0.000904941582120955 and test accuracy: 0.006378775462508202\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "7th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.05684, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.05684 to 0.00345, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00345\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00345\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00345 to 0.00089, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00089\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00089\n",
            "Epoch 00025: early stopping\n",
            "7-th combination = (False, False, False, 256, 64, 0.2) \n",
            " train accuracy: 0.0012773358030244708 and test accuracy: 0.0096387704834342\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "8th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00611, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00611\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00611\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00611\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00611 to 0.00237, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00237 to 0.00073, saving model to best_model.h5\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00073\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00073\n",
            "Epoch 00029: early stopping\n",
            "8-th combination = (False, False, True, 128, 128, 0.1) \n",
            " train accuracy: 0.002615976147353649 and test accuracy: 0.018308648839592934\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "9th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00292, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00292\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00292\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00292 to 0.00142, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00142\n",
            "Epoch 00024: early stopping\n",
            "9-th combination = (False, False, True, 128, 128, 0.2) \n",
            " train accuracy: 0.003076366614550352 and test accuracy: 0.024103395640850067\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "10th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.03041, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.03041 to 0.02910, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.02910 to 0.00463, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00463 to 0.00079, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00079\n",
            "Epoch 00024: early stopping\n",
            "10-th combination = (False, False, True, 128, 64, 0.1) \n",
            " train accuracy: 0.0023501364048570395 and test accuracy: 0.01991022191941738\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "11th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.03535, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.03535 to 0.01226, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.01226 to 0.00622, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00622\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00622 to 0.00168, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00168\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00168\n",
            "Epoch 00025: early stopping\n",
            "11-th combination = (False, False, True, 128, 64, 0.2) \n",
            " train accuracy: 0.00313945347443223 and test accuracy: 0.028869088739156723\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "12th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.01774, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01774\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01774\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.01774 to 0.00498, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00498\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00498\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00498 to 0.00079, saving model to best_model.h5\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00079\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00079\n",
            "Epoch 00027: early stopping\n",
            "12-th combination = (False, False, True, 256, 128, 0.1) \n",
            " train accuracy: 0.0022381492890417576 and test accuracy: 0.016355054453015327\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "13th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.01033, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01033\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01033\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.01033 to 0.00709, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00709\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00709\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00709\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00709 to 0.00136, saving model to best_model.h5\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00136\n",
            "Epoch 00028: early stopping\n",
            "13-th combination = (False, False, True, 256, 128, 0.2) \n",
            " train accuracy: 0.002129635075107217 and test accuracy: 0.01435360312461853\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "14th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.14823, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.14823 to 0.00732, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00732\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00732 to 0.00148, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00148\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00148\n",
            "Epoch 00024: early stopping\n",
            "14-th combination = (False, False, True, 256, 64, 0.1) \n",
            " train accuracy: 0.002300810068845749 and test accuracy: 0.018243923783302307\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "15th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.13042, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.13042 to 0.00248, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00248\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00248 to 0.00173, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00173\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00173\n",
            "Epoch 00024: early stopping\n",
            "15-th combination = (False, False, True, 256, 64, 0.2) \n",
            " train accuracy: 0.0027799191884696484 and test accuracy: 0.02204008586704731\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "16th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00432, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00432\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00432\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00432\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00432 to 0.00262, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00262\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00262\n",
            "Epoch 00025: early stopping\n",
            "16-th combination = (False, True, False, 128, 128, 0.1) \n",
            " train accuracy: 0.00391668314114213 and test accuracy: 0.030558854341506958\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "17th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.01556, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01556\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01556\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.01556 to 0.01100, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01100 to 0.00147, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00147\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00147\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00147 to 0.00143, saving model to best_model.h5\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00143\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00143\n",
            "Epoch 00028: early stopping\n",
            "17-th combination = (False, True, False, 128, 128, 0.2) \n",
            " train accuracy: 0.003986215218901634 and test accuracy: 0.03377372771501541\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "18th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.03292, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.03292\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.03292 to 0.00344, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00344\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00344 to 0.00219, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00219\n",
            "Epoch 00025: early stopping\n",
            "18-th combination = (False, True, False, 128, 64, 0.1) \n",
            " train accuracy: 0.006096496246755123 and test accuracy: 0.05770260468125343\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "19th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.03416, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.03416\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.03416 to 0.00517, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00517\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00517 to 0.00434, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00434\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00434\n",
            "Epoch 00025: early stopping\n",
            "19-th combination = (False, True, False, 128, 64, 0.2) \n",
            " train accuracy: 0.006702343933284283 and test accuracy: 0.0702388659119606\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "20th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.03670, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.03670\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.03670 to 0.02284, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.02284 to 0.01667, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01667\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.01667 to 0.00686, saving model to best_model.h5\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00686 to 0.00105, saving model to best_model.h5\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00105\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00105\n",
            "Epoch 00027: early stopping\n",
            "20-th combination = (False, True, False, 256, 128, 0.1) \n",
            " train accuracy: 0.00347931613214314 and test accuracy: 0.02702023833990097\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "21th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.02580, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.02580\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.02580 to 0.00660, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00660\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00660\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00660 to 0.00165, saving model to best_model.h5\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00165\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00165\n",
            "Epoch 00026: early stopping\n",
            "21-th combination = (False, True, False, 256, 128, 0.2) \n",
            " train accuracy: 0.0037549217231571674 and test accuracy: 0.030267057940363884\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "22th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.22587, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.22587 to 0.14743, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.14743 to 0.02257, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.02257\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.02257 to 0.00237, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00237\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00237\n",
            "Epoch 00025: early stopping\n",
            "22-th combination = (False, True, False, 256, 64, 0.1) \n",
            " train accuracy: 0.004217442125082016 and test accuracy: 0.036967918276786804\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "23th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.20102, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.20102 to 0.10956, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.10956 to 0.02406, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.02406\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.02406 to 0.00211, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00211\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00211\n",
            "Epoch 00025: early stopping\n",
            "23-th combination = (False, True, False, 256, 64, 0.2) \n",
            " train accuracy: 0.005142295267432928 and test accuracy: 0.04604780301451683\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "24th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.01253, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01253\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01253\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01253\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01253 to 0.01247, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01247\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01247\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.01247 to 0.01051, saving model to best_model.h5\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.01051 to 0.00145, saving model to best_model.h5\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00145\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00145\n",
            "Epoch 00029: early stopping\n",
            "24-th combination = (False, True, True, 128, 128, 0.1) \n",
            " train accuracy: 0.005983831826597452 and test accuracy: 0.055549006909132004\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "25th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00540, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00540\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00540\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00540 to 0.00219, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00219\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00219\n",
            "Epoch 00024: early stopping\n",
            "25-th combination = (False, True, True, 128, 128, 0.2) \n",
            " train accuracy: 0.00513409124687314 and test accuracy: 0.04458633437752724\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "26th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.08517, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.08517 to 0.03037, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.03037 to 0.02005, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.02005 to 0.01598, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01598 to 0.00608, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00608\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00608\n",
            "Epoch 00025: early stopping\n",
            "26-th combination = (False, True, True, 128, 64, 0.1) \n",
            " train accuracy: 0.00914095900952816 and test accuracy: 0.09753862768411636\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "27th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.07791, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.07791 to 0.07565, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.07565 to 0.00584, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00584\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00584 to 0.00284, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00284\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00284\n",
            "Epoch 00025: early stopping\n",
            "27-th combination = (False, True, True, 128, 64, 0.2) \n",
            " train accuracy: 0.009078595787286758 and test accuracy: 0.09208660572767258\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "28th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.06498, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.06498\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.06498\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.06498 to 0.03624, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.03624\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.03624\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.03624 to 0.03406, saving model to best_model.h5\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.03406 to 0.00488, saving model to best_model.h5\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00488\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00488\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00488\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00488\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00488\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00488\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00488\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.00488 to 0.00455, saving model to best_model.h5\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00455\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00455\n",
            "Epoch 00036: early stopping\n",
            "28-th combination = (False, True, True, 256, 128, 0.1) \n",
            " train accuracy: 0.005867676343768835 and test accuracy: 0.05446648970246315\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "29th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.06255, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.06255\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.06255\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.06255 to 0.02272, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.02272\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.02272\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.02272 to 0.00175, saving model to best_model.h5\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00175\n",
            "Epoch 00027: early stopping\n",
            "29-th combination = (False, True, True, 256, 128, 0.2) \n",
            " train accuracy: 0.0045529515482485294 and test accuracy: 0.03590526059269905\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "30th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.20121, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.20121 to 0.01493, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01493\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.01493 to 0.00384, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00384\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00384\n",
            "Epoch 00024: early stopping\n",
            "30-th combination = (False, True, True, 256, 64, 0.1) \n",
            " train accuracy: 0.005090897437185049 and test accuracy: 0.050082869827747345\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "31th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.21870, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.21870 to 0.02039, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.02039\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.02039 to 0.00157, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00157\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00157\n",
            "Epoch 00024: early stopping\n",
            "31-th combination = (False, True, True, 256, 64, 0.2) \n",
            " train accuracy: 0.004813882056623697 and test accuracy: 0.04511140659451485\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "32th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00349, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00349\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00349\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00349\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00349\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00349\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00349\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00349 to 0.00139, saving model to best_model.h5\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00139\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00139\n",
            "Epoch 00028: early stopping\n",
            "32-th combination = (True, False, False, 128, 128, 0.1) \n",
            " train accuracy: 0.00401405431330204 and test accuracy: 0.03190511837601662\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "33th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00546, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00546\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00546\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00546 to 0.00188, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00188\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00188\n",
            "Epoch 00024: early stopping\n",
            "33-th combination = (True, False, False, 128, 128, 0.2) \n",
            " train accuracy: 0.0035220603458583355 and test accuracy: 0.028680406510829926\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "34th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.03421, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.03421 to 0.03367, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.03367 to 0.00540, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00540\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00540 to 0.00161, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00161\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00161\n",
            "Epoch 00025: early stopping\n",
            "34-th combination = (True, False, False, 128, 64, 0.1) \n",
            " train accuracy: 0.005912421736866236 and test accuracy: 0.05865800380706787\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "35th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.02760, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.02760\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.02760 to 0.00816, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00816 to 0.00531, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00531\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00531\n",
            "Epoch 00024: early stopping\n",
            "35-th combination = (True, False, False, 128, 64, 0.2) \n",
            " train accuracy: 0.00688583729788661 and test accuracy: 0.0691085234284401\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "36th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.01735, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01735\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01735\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.01735 to 0.00567, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00567\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00567\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00567 to 0.00108, saving model to best_model.h5\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00108\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00108\n",
            "Epoch 00027: early stopping\n",
            "36-th combination = (True, False, False, 256, 128, 0.1) \n",
            " train accuracy: 0.0033607976511120796 and test accuracy: 0.028452353551983833\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "37th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.02998, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.02998\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.02998\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.02998 to 0.00892, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00892\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00892\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00892 to 0.00149, saving model to best_model.h5\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00149\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00149\n",
            "Epoch 00027: early stopping\n",
            "37-th combination = (True, False, False, 256, 128, 0.2) \n",
            " train accuracy: 0.004477585665881634 and test accuracy: 0.034880850464105606\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "38th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.08843, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.08843 to 0.01713, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.01713 to 0.00098, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00098\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00098\n",
            "Epoch 00023: early stopping\n",
            "38-th combination = (True, False, False, 256, 64, 0.1) \n",
            " train accuracy: 0.004391020629554987 and test accuracy: 0.04252883791923523\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "39th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.08858, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.08858 to 0.00665, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00665 to 0.00354, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00354\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00354\n",
            "Epoch 00023: early stopping\n",
            "39-th combination = (True, False, False, 256, 64, 0.2) \n",
            " train accuracy: 0.002981071826070547 and test accuracy: 0.02921098656952381\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "40th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00607, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00607\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00607\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00607\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00607 to 0.00537, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00537\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00537\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00537 to 0.00205, saving model to best_model.h5\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00205\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00205\n",
            "Epoch 00028: early stopping\n",
            "40-th combination = (True, False, True, 128, 128, 0.1) \n",
            " train accuracy: 0.0059249685145914555 and test accuracy: 0.05556022748351097\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "41th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00425, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00425\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00425\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00425 to 0.00210, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00210\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00210\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00210 to 0.00199, saving model to best_model.h5\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00199\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00199\n",
            "Epoch 00027: early stopping\n",
            "41-th combination = (True, False, True, 128, 128, 0.2) \n",
            " train accuracy: 0.005798263009637594 and test accuracy: 0.051487863063812256\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "42th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.08120, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.08120 to 0.00275, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00275\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00275 to 0.00183, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00183\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00183\n",
            "Epoch 00024: early stopping\n",
            "42-th combination = (True, False, True, 128, 64, 0.1) \n",
            " train accuracy: 0.009308064356446266 and test accuracy: 0.09629552066326141\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "43th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.07414, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.07414\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.07414 to 0.01198, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01198\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01198 to 0.00334, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00334\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00334\n",
            "Epoch 00025: early stopping\n",
            "43-th combination = (True, False, True, 128, 64, 0.2) \n",
            " train accuracy: 0.008971544913947582 and test accuracy: 0.08842287212610245\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "44th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.05582, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.05582\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.05582\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.05582 to 0.03523, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.03523\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.03523\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.03523 to 0.01134, saving model to best_model.h5\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.01134 to 0.00141, saving model to best_model.h5\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00141\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00141\n",
            "Epoch 00028: early stopping\n",
            "44-th combination = (True, False, True, 256, 128, 0.1) \n",
            " train accuracy: 0.005176347214728594 and test accuracy: 0.04268110543489456\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "45th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.03862, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.03862\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.03862\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.03862 to 0.01327, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01327\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01327\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.01327 to 0.00185, saving model to best_model.h5\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00185\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00185\n",
            "Epoch 00027: early stopping\n",
            "45-th combination = (True, False, True, 256, 128, 0.2) \n",
            " train accuracy: 0.005015797913074493 and test accuracy: 0.039666369557380676\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "46th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.24166, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.24166 to 0.04491, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.04491\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.04491 to 0.00142, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00142\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00142\n",
            "Epoch 00024: early stopping\n",
            "46-th combination = (True, False, True, 256, 64, 0.1) \n",
            " train accuracy: 0.005655639339238405 and test accuracy: 0.054568421095609665\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "47th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.25164, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.25164 to 0.11211, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.11211 to 0.07559, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.07559 to 0.02142, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.02142 to 0.01396, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01396\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01396\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01396\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.01396 to 0.00688, saving model to best_model.h5\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00688\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00688\n",
            "Epoch 00029: early stopping\n",
            "47-th combination = (True, False, True, 256, 64, 0.2) \n",
            " train accuracy: 0.006456305272877216 and test accuracy: 0.06539156287908554\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "48th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.01207, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01207\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01207\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01207\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01207\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01207\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01207\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01207\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.01207 to 0.00230, saving model to best_model.h5\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00230\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00230\n",
            "Epoch 00029: early stopping\n",
            "48-th combination = (True, True, False, 128, 128, 0.1) \n",
            " train accuracy: 0.008702867664396763 and test accuracy: 0.0821952074766159\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "49th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.01004, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01004\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01004\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.01004 to 0.00551, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00551\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00551\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00551 to 0.00333, saving model to best_model.h5\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00333\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00333\n",
            "Epoch 00027: early stopping\n",
            "49-th combination = (True, True, False, 128, 128, 0.2) \n",
            " train accuracy: 0.011477860622107983 and test accuracy: 0.11760058254003525\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "50th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.06902, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.06902 to 0.03155, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.03155 to 0.01780, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.01780 to 0.00712, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00712\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00712\n",
            "Epoch 00024: early stopping\n",
            "50-th combination = (True, True, False, 128, 64, 0.1) \n",
            " train accuracy: 0.01334844809025526 and test accuracy: 0.14993436634540558\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "51th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.06730, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.06730 to 0.03947, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.03947 to 0.02255, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.02255 to 0.01957, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01957 to 0.00911, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00911\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00911\n",
            "Epoch 00025: early stopping\n",
            "51-th combination = (True, True, False, 128, 64, 0.2) \n",
            " train accuracy: 0.012412854470312595 and test accuracy: 0.1396312266588211\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "52th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.07344, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.07344\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.07344 to 0.03341, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.03341\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.03341\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.03341 to 0.00169, saving model to best_model.h5\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00169\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00169\n",
            "Epoch 00026: early stopping\n",
            "52-th combination = (True, True, False, 256, 128, 0.1) \n",
            " train accuracy: 0.007242434658110142 and test accuracy: 0.0683261975646019\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "53th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.05984, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.05984\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.05984\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.05984 to 0.02268, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.02268\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.02268\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.02268 to 0.00242, saving model to best_model.h5\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00242\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00242\n",
            "Epoch 00027: early stopping\n",
            "53-th combination = (True, True, False, 256, 128, 0.2) \n",
            " train accuracy: 0.009064171463251114 and test accuracy: 0.08342470973730087\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "54th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.22631, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.22631 to 0.05094, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.05094\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.05094 to 0.00786, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00786\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00786\n",
            "Epoch 00024: early stopping\n",
            "54-th combination = (True, True, False, 256, 64, 0.1) \n",
            " train accuracy: 0.00846890453249216 and test accuracy: 0.09280392527580261\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "55th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.23845, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.23845 to 0.06636, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.06636\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.06636 to 0.00382, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00382\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00382\n",
            "Epoch 00024: early stopping\n",
            "55-th combination = (True, True, False, 256, 64, 0.2) \n",
            " train accuracy: 0.010440060868859291 and test accuracy: 0.11228252202272415\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "56th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00929, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00929\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00929\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00929\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00929\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00929\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00929\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00929 to 0.00175, saving model to best_model.h5\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00175\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00175\n",
            "Epoch 00028: early stopping\n",
            "56-th combination = (True, True, True, 128, 128, 0.1) \n",
            " train accuracy: 0.0103581752628088 and test accuracy: 0.09859952330589294\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "57th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00825, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00825\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00825\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00825\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00825\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00825\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00825\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00825 to 0.00263, saving model to best_model.h5\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00263\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00263\n",
            "Epoch 00028: early stopping\n",
            "57-th combination = (True, True, True, 128, 128, 0.2) \n",
            " train accuracy: 0.010981163010001183 and test accuracy: 0.11415795236825943\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "58th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.12371, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.12371 to 0.12253, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.12253 to 0.01307, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01307\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01307 to 0.00585, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00585\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00585\n",
            "Epoch 00025: early stopping\n",
            "58-th combination = (True, True, True, 128, 64, 0.1) \n",
            " train accuracy: 0.015558206476271152 and test accuracy: 0.17596986889839172\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "59th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.13614, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.13614 to 0.13034, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.13034 to 0.01647, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01647\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01647 to 0.00194, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00194\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00194\n",
            "Epoch 00025: early stopping\n",
            "59-th combination = (True, True, True, 128, 64, 0.2) \n",
            " train accuracy: 0.016286013647913933 and test accuracy: 0.18283657729625702\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "60th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.09518, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.09518\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.09518\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.09518 to 0.06701, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.06701\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.06701\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.06701 to 0.03032, saving model to best_model.h5\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.03032 to 0.00220, saving model to best_model.h5\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00220\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00220\n",
            "Epoch 00028: early stopping\n",
            "60-th combination = (True, True, True, 256, 128, 0.1) \n",
            " train accuracy: 0.01029543112963438 and test accuracy: 0.09631557762622833\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "61th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.10513, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.10513\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.10513\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.10513 to 0.05137, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.05137\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.05137 to 0.01018, saving model to best_model.h5\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01018\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01018\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.01018 to 0.00997, saving model to best_model.h5\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00997\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00997\n",
            "Epoch 00032: early stopping\n",
            "61-th combination = (True, True, True, 256, 128, 0.2) \n",
            " train accuracy: 0.012627563439309597 and test accuracy: 0.13126243650913239\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "62th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.27993, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.27993 to 0.13144, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.13144 to 0.12838, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.12838 to 0.05600, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.05600 to 0.01679, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01679\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01679\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01679\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.01679 to 0.01619, saving model to best_model.h5\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.01619\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.01619\n",
            "Epoch 00029: early stopping\n",
            "62-th combination = (True, True, True, 256, 64, 0.1) \n",
            " train accuracy: 0.017514964565634727 and test accuracy: 0.19862069189548492\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "63th combination: \n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.29847, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.29847 to 0.17678, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.17678 to 0.10069, saving model to best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.10069 to 0.05166, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.05166 to 0.01508, saving model to best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.01508 to 0.00818, saving model to best_model.h5\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00818\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00818\n",
            "Epoch 00026: early stopping\n",
            "63-th combination = (True, True, True, 256, 64, 0.2) \n",
            " train accuracy: 0.012589046731591225 and test accuracy: 0.135016530752182\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er9P7-7antcL"
      },
      "source": [
        "<a id=\"44\"></a> <br>\n",
        "## Choosing the Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlXcldQKntcL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "9c40a887-3802-4f12-e600-85ff2007669c"
      },
      "source": [
        "hist = pd.DataFrame(hist)\n",
        "hist = hist.sort_values(by=[7], ascending=True)\n",
        "hist"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>256</td>\n",
              "      <td>64</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.000905</td>\n",
              "      <td>0.006379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>256</td>\n",
              "      <td>128</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001247</td>\n",
              "      <td>0.007802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>256</td>\n",
              "      <td>128</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.008859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>256</td>\n",
              "      <td>64</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001277</td>\n",
              "      <td>0.009639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001711</td>\n",
              "      <td>0.009903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.012413</td>\n",
              "      <td>0.139631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.013348</td>\n",
              "      <td>0.149934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.015558</td>\n",
              "      <td>0.175970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.016286</td>\n",
              "      <td>0.182837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>256</td>\n",
              "      <td>64</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.017515</td>\n",
              "      <td>0.198621</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>64 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0      1      2    3    4    5         6         7\n",
              "6   False  False  False  256   64  0.1  0.000905  0.006379\n",
              "5   False  False  False  256  128  0.2  0.001247  0.007802\n",
              "4   False  False  False  256  128  0.1  0.001400  0.008859\n",
              "7   False  False  False  256   64  0.2  0.001277  0.009639\n",
              "0   False  False  False  128  128  0.1  0.001711  0.009903\n",
              "..    ...    ...    ...  ...  ...  ...       ...       ...\n",
              "51   True   True  False  128   64  0.2  0.012413  0.139631\n",
              "50   True   True  False  128   64  0.1  0.013348  0.149934\n",
              "58   True   True   True  128   64  0.1  0.015558  0.175970\n",
              "59   True   True   True  128   64  0.2  0.016286  0.182837\n",
              "62   True   True   True  256   64  0.1  0.017515  0.198621\n",
              "\n",
              "[64 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cPtfgbVntcL"
      },
      "source": [
        "<a id=\"5\"></a> <br>\n",
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYm00ULSntcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d9f359-1d7b-4654-9782-ac6e734ad339"
      },
      "source": [
        "print(f'Best Combination: \\n first_additional_layer = {hist.iloc[0, 0]}\\n second_additional_layer = {hist.iloc[0, 1]}\\n third_additional_layer = {hist.iloc[0, 2]}\\n n_neurons = {hist.iloc[0, 3]}\\n n_batch_size = {hist.iloc[0, 4]}\\n dropout = {hist.iloc[0, 5]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Combination: \n",
            " first_additional_layer = False\n",
            " second_additional_layer = False\n",
            " third_additional_layer = False\n",
            " n_neurons = 256\n",
            " n_batch_size = 64\n",
            " dropout = 0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP4zbWuLntcM"
      },
      "source": [
        "first_additional_layer, second_additional_layer, third_additional_layer, n_neurons, n_batch_size, dropout = list(hist.iloc[0, :-2])\n",
        "# print(first_additional_layer)\n",
        "# print(second_additional_layer)\n",
        "# print(third_additional_layer)\n",
        "# print(n_neurons)\n",
        "# print( n_batch_size)\n",
        "# print( dropout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxpYuas-ntcM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946c979e-7f81-49d1-dac3-321eb21e4031"
      },
      "source": [
        "regressor = Sequential()\n",
        "regressor.add(LSTM(units=n_neurons, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "regressor.add(Dropout(dropout))\n",
        "\n",
        "if first_additional_layer:\n",
        "    regressor.add(LSTM(units=n_neurons, return_sequences=True))\n",
        "    regressor.add(Dropout(dropout))\n",
        "\n",
        "if second_additional_layer:\n",
        "    regressor.add(LSTM(units=n_neurons, return_sequences=True))\n",
        "    regressor.add(Dropout(dropout))\n",
        "\n",
        "if third_additional_layer:\n",
        "    regressor.add(GRU(units=n_neurons, return_sequences=True))\n",
        "    regressor.add(Dropout(dropout))\n",
        "\n",
        "regressor.add(LSTM(units=n_neurons, return_sequences=False))\n",
        "regressor.add(Dropout(dropout))\n",
        "regressor.add(Dense(units=1, activation='linear'))\n",
        "regressor.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
        "\n",
        "file_path = 'best_model.h5'\n",
        "\n",
        "mc = ModelCheckpoint(file_path, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "\n",
        "regressor.fit(X_train, y_train, validation_split=0.3, epochs=100, batch_size=n_batch_size, callbacks=[es, mc], verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.04172, saving model to best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.04172 to 0.00072, saving model to best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00072\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00072 to 0.00056, saving model to best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00056\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00056\n",
            "Epoch 00024: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0478a3a350>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGoayVfUaclp",
        "outputId": "0bb81b33-828c-498a-a287-2553cad226c3"
      },
      "source": [
        "regressor.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 49ms/step - loss: 0.0073\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0073105040937662125"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "id": "y9NGft_gahqc",
        "outputId": "a8b1a71c-d7cd-40ab-c363-573c4bd14313"
      },
      "source": [
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "plt.figure(figsize=(16,8), dpi= 100, facecolor='w', edgecolor='k')\n",
        "\n",
        "plt.plot(y_test, color='red', label = 'Real cases of covid')\n",
        "plt.plot(y_pred, color='green', label = 'Predicted covid cases')\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f04786fafd0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1600x800 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is6T5rCial6p"
      },
      "source": [
        "data = pd.DataFrame(y_pred, columns = ['Column_A'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "XZiF_bapapkl",
        "outputId": "2bbecf88-65ab-4d60-8cdb-db76e1ea7f51"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Column_A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.835517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.836786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.838024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.839235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.840422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>0.906465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.906968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.907490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.908020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.908531</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Column_A\n",
              "0   0.835517\n",
              "1   0.836786\n",
              "2   0.838024\n",
              "3   0.839235\n",
              "4   0.840422\n",
              "..       ...\n",
              "94  0.906465\n",
              "95  0.906968\n",
              "96  0.907490\n",
              "97  0.908020\n",
              "98  0.908531\n",
              "\n",
              "[99 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iPLR2kThKlj",
        "outputId": "b4c8f28b-0d7a-4f6b-fd5f-7849e72ad26d"
      },
      "source": [
        "d1=sc.inverse_transform(data)\n",
        "d1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[23114.064],\n",
              "       [23149.154],\n",
              "       [23183.402],\n",
              "       [23216.898],\n",
              "       [23249.723],\n",
              "       [23281.803],\n",
              "       [23312.982],\n",
              "       [23342.873],\n",
              "       [23371.324],\n",
              "       [23398.182],\n",
              "       [23423.432],\n",
              "       [23447.018],\n",
              "       [23469.105],\n",
              "       [23489.873],\n",
              "       [23509.492],\n",
              "       [23528.314],\n",
              "       [23546.715],\n",
              "       [23565.021],\n",
              "       [23583.44 ],\n",
              "       [23601.912],\n",
              "       [23620.61 ],\n",
              "       [23640.205],\n",
              "       [23661.295],\n",
              "       [23683.844],\n",
              "       [23707.693],\n",
              "       [23732.395],\n",
              "       [23757.572],\n",
              "       [23782.795],\n",
              "       [23807.715],\n",
              "       [23832.379],\n",
              "       [23856.926],\n",
              "       [23881.164],\n",
              "       [23904.852],\n",
              "       [23927.879],\n",
              "       [23950.354],\n",
              "       [23972.148],\n",
              "       [23993.562],\n",
              "       [24014.885],\n",
              "       [24037.945],\n",
              "       [24062.883],\n",
              "       [24090.188],\n",
              "       [24119.914],\n",
              "       [24151.523],\n",
              "       [24184.05 ],\n",
              "       [24216.951],\n",
              "       [24249.135],\n",
              "       [24279.775],\n",
              "       [24308.451],\n",
              "       [24334.967],\n",
              "       [24359.46 ],\n",
              "       [24383.154],\n",
              "       [24406.857],\n",
              "       [24431.318],\n",
              "       [24456.883],\n",
              "       [24483.656],\n",
              "       [24511.225],\n",
              "       [24539.018],\n",
              "       [24566.469],\n",
              "       [24593.143],\n",
              "       [24618.924],\n",
              "       [24643.645],\n",
              "       [24667.38 ],\n",
              "       [24690.186],\n",
              "       [24712.072],\n",
              "       [24733.096],\n",
              "       [24753.496],\n",
              "       [24773.398],\n",
              "       [24792.824],\n",
              "       [24811.873],\n",
              "       [24830.617],\n",
              "       [24848.9  ],\n",
              "       [24866.865],\n",
              "       [24884.637],\n",
              "       [24902.238],\n",
              "       [24919.648],\n",
              "       [24936.932],\n",
              "       [24954.102],\n",
              "       [24971.04 ],\n",
              "       [24987.766],\n",
              "       [25004.346],\n",
              "       [25016.855],\n",
              "       [25024.592],\n",
              "       [25027.93 ],\n",
              "       [25027.762],\n",
              "       [25024.818],\n",
              "       [25020.994],\n",
              "       [25018.012],\n",
              "       [25017.146],\n",
              "       [25018.94 ],\n",
              "       [25023.48 ],\n",
              "       [25030.668],\n",
              "       [25039.963],\n",
              "       [25050.941],\n",
              "       [25063.213],\n",
              "       [25076.467],\n",
              "       [25090.361],\n",
              "       [25104.807],\n",
              "       [25119.457],\n",
              "       [25133.596]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e0B3lz2ntcN"
      },
      "source": [
        "data.to_csv('data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bxZ6LNGntcN"
      },
      "source": [
        "data1 = pd.DataFrame(y_test, columns = ['Column_A'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0zH8QMpntcN"
      },
      "source": [
        "data1.to_csv('data1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIic_gwn-OI4"
      },
      "source": [
        "## **Plotting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYlfI2hkB4gT"
      },
      "source": [
        "d3 = regressor.predict(X_train)\n",
        "d3 = sc.inverse_transform(d3)\n",
        "y_train= sc.inverse_transform(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55gXuwxn_OOb"
      },
      "source": [
        "d2=sc.inverse_transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "AgyD1wS5-Q1J",
        "outputId": "23d1bf5b-d107-4e8c-c505-305660bbc8bf"
      },
      "source": [
        "def plot_future(prediction, model_name, y_test):\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    range_future = len(prediction)\n",
        "\n",
        "    plt.plot(np.arange(range_future), np.array(y_test), label='True Future')\n",
        "    plt.plot(np.arange(range_future), np.array(prediction),label='Prediction')\n",
        "\n",
        "    plt.title('True future vs prediction for ' + model_name)\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xlabel('No of Days')\n",
        "    plt.ylabel('Covid Cases')\n",
        "    #plt.savefig('C:/Users/nious/Documents/Medium/LSTM&GRU/predic_'+model_name+'.jpg', format='jpg', dpi=1000)\n",
        "    \n",
        "plot_future(d3, 'Tunned LSTM', y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "9Ainz9F-DoCQ",
        "outputId": "1d3f0095-1c15-484f-dad0-471345d6e82f"
      },
      "source": [
        "plot_future(d1, 'Tunned LSTM', d2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l_E23SO4JIL"
      },
      "source": [
        "# **Errors**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFKQdkXC4OuM"
      },
      "source": [
        "### **(i) Root mean Square error**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7iI9YkV4Udy",
        "outputId": "cfabd9d8-9f1e-418f-d03c-9c7e508170c9"
      },
      "source": [
        "from math import sqrt\n",
        "\n",
        "# Calculate root mean squared error\n",
        "def rmse_metric(actual, predicted):\n",
        "\tsum_error = 0.0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tprediction_error = predicted[i] - actual[i]\n",
        "\t\tsum_error += (prediction_error ** 2)\n",
        "\tmean_error = sum_error / float(len(actual))\n",
        "\treturn sqrt(mean_error)\n",
        "\n",
        "# Test RMSE\n",
        "rmse = rmse_metric(y_test, y_pred)\n",
        "print(rmse)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.08550148616112767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L4Bhjg_4U7a"
      },
      "source": [
        "### **(ii) Mean Absolute Error** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_OqJ2u24e2E",
        "outputId": "92df32cf-2c03-4bcc-9d49-5dc421f09782"
      },
      "source": [
        "# Calculate mean absolute error\n",
        "def mae_metric(actual, predicted):\n",
        "\tsum_error = 0.0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tsum_error += abs(predicted[i] - actual[i])\n",
        "\treturn sum_error / float(len(actual))\n",
        "\n",
        "# Test RMSE\n",
        "mae = mae_metric(y_test, y_pred)\n",
        "print(mae)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.08537041]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNEHlKV54fUg"
      },
      "source": [
        "### **(iii) Mean squared Error**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-aeybKe4lVX",
        "outputId": "7a7ba37f-48cc-413f-ed41-ed7ccb5f40d4"
      },
      "source": [
        "MSE= np.square(np.subtract(y_test,y_pred)).mean()\n",
        "print(MSE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.007310504135761505\n"
          ]
        }
      ]
    }
  ]
}